{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Real-Time Fraud Detection Demo (No Kafka)\n", "\n", "This notebook simulates real-time transaction ingestion and dashboarding using Delta Lake and Structured Streaming in Databricks.\n", "\n", "**Demo Highlights:**\n", "- Simulated real-time data feed\n", "- Live-updating fraud dashboards\n", "- Delta Lake for reliable streaming\n", "- (Optional) Feature engineering & inference"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Step 1: Initialize Delta table for streaming\n", "from pyspark.sql.types import StructType, IntegerType, StringType, TimestampType\n", "\n", "schema = StructType() \\\n", "    .add(\"amount\", IntegerType()) \\\n", "    .add(\"merchant_code\", IntegerType()) \\\n", "    .add(\"channel\", StringType()) \\\n", "    .add(\"fraud\", IntegerType()) \\\n", "    .add(\"timestamp\", TimestampType())\n", "\n", "# Create empty base table\n", "spark.sql(\"DROP TABLE IF EXISTS transaction_stream\")\n", "spark.createDataFrame([], schema).write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"transaction_stream\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 2: Run this code in a separate notebook to simulate real-time data feed\n", "_Keep this running in the background during the demo._"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import random\n", "import time\n", "from datetime import datetime\n", "from pyspark.sql import Row\n", "\n", "channels = [\"online\", \"instore\"]\n", "\n", "def generate_transaction():\n", "    return Row(\n", "        amount=random.randint(1, 1000),\n", "        merchant_code=random.randint(0, 4),\n", "        channel=random.choice(channels),\n", "        fraud=int(random.random() < 0.04),\n", "        timestamp=datetime.now()\n", "    )\n", "\n", "while True:\n", "    rows = [generate_transaction() for _ in range(10)]\n", "    df = spark.createDataFrame(rows)\n", "    df.write.format(\"delta\").mode(\"append\").saveAsTable(\"transaction_stream\")\n", "    time.sleep(5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 3: Visualize live data in this notebook\n", "_Open multiple displays to simulate dashboards._"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Real-time fraud counts\n", "df_stream = spark.readStream.format(\"delta\").table(\"transaction_stream\")\n", "df_stream.groupBy(\"fraud\").count().display()"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Fraud rate by channel\n", "df_stream.groupBy(\"channel\", \"fraud\").count().orderBy(\"channel\").display()"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Real-time histogram of amounts\n", "df_stream.select(\"amount\").display()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 4: Feature Engineering and Model Scoring (Optional)\n", "_Enrich the stream with features and score transactions in real time._"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["from pyspark.sql.functions import when\n", "\n", "# Add a 'high_amount' feature for scoring\n", "df_features = df_stream.withColumn(\"high_amount\", when(df_stream.amount > 500, 1).otherwise(0))\n", "df_features.display()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 5: Model Registration with MLflow\n", "_This part assumes you have already trained and logged a model._"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Example of logging model metrics (after training)\n", "import mlflow\n", "\n", "with mlflow.start_run():\n", "    mlflow.log_metric(\"stream_precision\", 0.82)\n", "    mlflow.log_metric(\"stream_recall\", 0.67)\n", "    print(\"Metrics logged to MLflow.\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 6: Real-Time Model Serving (Optional)\n", "_Invoke a deployed model endpoint to score new transactions._"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import requests\n", "import json\n", "\n", "# Replace <your-endpoint-url> and <your-token>\n", "endpoint = \"https://<your-databricks-workspace>/model/FraudDetectionModel/production/invocations\"\n", "headers = {\n", "    \"Authorization\": f\"Bearer <your-token>\",\n", "    \"Content-Type\": \"application/json\"\n", "}\n", "\n", "# Example payload\n", "payload = {\n", "  \"dataframe_split\": {\n", "    \"columns\": [\"amount\", \"merchant_code\", \"high_amount\"],\n", "    \"data\": [[950, 2, 1]]\n", "  }\n", "}\n", "\n", "response = requests.post(endpoint, headers=headers, json=payload)\n", "print(\"Prediction:\", response.json())"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8"}}, "nbformat": 4, "nbformat_minor": 2}